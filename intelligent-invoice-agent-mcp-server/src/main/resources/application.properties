spring.application.name=intelligent-invoice-agent-mcp-server

# Web Server Configuration
spring.main.web-application-type=servlet
spring.main.banner-mode=off
spring.ai.mcp.server.name=intelligent-invoice-agent-mcp-server
spring.ai.mcp.server.version=0.0.1

llm.api.key=${LLM_API_KEY:}
# Default provider URL (change if you use Groq)
llm.api.url=${LLM_API_URL:https://api.openai.com/v1/chat/completions}
# Force a supported model to avoid using decommissioned models from environment variables
llm.model=gpt-4o-mini

# Logging - enable debug for our project to see detailed LLM logs
logging.level.root=INFO
logging.level.com.anupam=DEBUG


# Python API Configuration
python.api.url=http://localhost:8000/process-invoice
